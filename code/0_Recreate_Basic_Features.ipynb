{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate Basic Features\n",
    "\n",
    "The following features are already given in OnlineNewsPopularity data file. \n",
    "This notebook tries to recreate them as much as possible\n",
    "\n",
    "* n_tokens_title\n",
    "* n_tokens_content\n",
    "* n_unique_tokens\n",
    "* n_non_stop_words\n",
    "* n_non_stop_unique_tokens\n",
    "* num_hrefs\n",
    "* num_self_hrefs\n",
    "* num_imgs\n",
    "* num_videos\n",
    "* average_token_length\n",
    "* num_keywords\n",
    "* data_channel_is_lifestyle\n",
    "* data_channel_is_entertainment\n",
    "* data_channel_is_bus\n",
    "* data_channel_is_socmed\n",
    "* data_channel_is_tech\n",
    "* data_channel_is_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NLP actions\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import element\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = pd.read_excel('../data/output/SS_Extracted_content.xlsx')[[\"Id\"]]\n",
    "df_in = df_in[ df_in.Id < 20]\n",
    "df_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80949\n",
      "<!DOCTYPE html>\n",
      "<html data-env='production' lang='en' xml:lang='en'>\n",
      "<head>\n",
      "<script>\n",
      "  window.__o = \n",
      ".....\n",
      " document.getElementsByTagName('body')[0]).appendChild(s);\n",
      "    })();\n",
      "  }\n",
      "</script>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = df_in.Id[13]\n",
    "\n",
    "with open( \"../data/output/html/\" + str(doc_id) + \".html\", \"r\", encoding=\"utf-8\", errors='ignore') as html_file:\n",
    "    html_doc = html_file.read()\n",
    "\n",
    "print( len(html_doc))\n",
    "print( html_doc[:100])\n",
    "print( \".....\")\n",
    "print( html_doc[-100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                               int32\n",
       "n_tokens_title                   int32\n",
       "n_tokens_content                 int32\n",
       "n_unique_tokens                  int32\n",
       "n_non_stop_words                 int32\n",
       "n_non_stop_unique_tokens         int32\n",
       "num_hrefs                        int32\n",
       "num_self_hrefs                   int32\n",
       "num_imgs                         int32\n",
       "num_videos                       int32\n",
       "average_token_length             int32\n",
       "num_keywords                     int32\n",
       "data_channel_is_lifestyle        int32\n",
       "data_channel_is_entertainment    int32\n",
       "data_channel_is_bus              int32\n",
       "data_channel_is_socmed           int32\n",
       "data_channel_is_tech             int32\n",
       "data_channel_is_world            int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = pd.DataFrame(columns=[\"Id\", \"n_tokens_title\", \"n_tokens_content\", \"n_unique_tokens\", \\\n",
    "                               \"n_non_stop_words\", \"n_non_stop_unique_tokens\", \"num_hrefs\", \\\n",
    "                               \"num_self_hrefs\", \"num_imgs\", \"num_videos\", \"average_token_length\", \\\n",
    "                               \"num_keywords\", \"data_channel_is_lifestyle\", \\\n",
    "                               \"data_channel_is_entertainment\", \"data_channel_is_bus\", \\\n",
    "                               \"data_channel_is_socmed\", \"data_channel_is_tech\", \"data_channel_is_world\"])\n",
    "df_out = df_out.astype( {\"Id\":int, \"n_tokens_title\":int, \"n_tokens_content\":int, \"n_unique_tokens\":int, \\\n",
    "                         \"n_non_stop_words\":int, \"n_non_stop_unique_tokens\":int, \"num_hrefs\":int, \\\n",
    "                         \"num_self_hrefs\":int, \"num_imgs\":int, \"num_videos\":int, \"average_token_length\":int, \\\n",
    "                         \"num_keywords\":int, \"data_channel_is_lifestyle\":int, \"data_channel_is_entertainment\":int, \\\n",
    "                         \"data_channel_is_bus\":int, \"data_channel_is_socmed\":int, \"data_channel_is_tech\":int, \\\n",
    "                         \"data_channel_is_world\":int})\n",
    "\n",
    "df_out.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Glass is Getting a Second Look from Businesses\n",
      "n_tokens_title :  9\n"
     ]
    }
   ],
   "source": [
    "# Calculate title features\n",
    "title = str( soup.title.string)\n",
    "nlp_title = nlp(title)\n",
    "\n",
    "n_tokens_title = 0\n",
    "for token in nlp_title:\n",
    "    n_tokens_title += 1\n",
    "    \n",
    "print( title)\n",
    "print( \"n_tokens_title : \" , n_tokens_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract content and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_hrefs :  12\n",
      "num_self_hrefs :  5\n",
      "num_imgs :  1\n",
      "num_videos :  1\n"
     ]
    }
   ],
   "source": [
    "# the contents containing repeating / unnecessary info - these classes are excluded\n",
    "exclude_class_list = [ \"top-stories-promo-story__summary\"]\n",
    "\n",
    "exclude_starts_with = [\"Additional reporting by\"]\n",
    "regex_keyword = \"\"\"<meta content=\"(?P<keyword1>[^><\\/\\\"]*)\"\\s[a-zA-Z=\"\\-]*\\sname=\"keywords\".?.?>\"\"\"\n",
    "\n",
    "content = \"\" # soup.title.string + \"\\n\"\n",
    "\n",
    "num_hrefs = 0\n",
    "num_self_hrefs = 0\n",
    "num_imgs = 0\n",
    "num_videos = 0\n",
    "\n",
    "for p in soup.select( \"p\"):\n",
    "    for child in p.children:\n",
    "        if isinstance( child, element.Tag):\n",
    "            type_of_tag = child.name\n",
    "            \n",
    "            if not child.name is None:\n",
    "                if child.name == \"a\":\n",
    "                    num_hrefs += 1\n",
    "                    \n",
    "                    href_url = child.get('href')\n",
    "                    if (not href_url is None) and (\"mashable.com\" in href_url.lower()):\n",
    "                        num_self_hrefs += 1\n",
    "                        \n",
    "                if child.name == \"img\" or child.name == \"picture\":\n",
    "                    num_imgs += 1\n",
    "                    \n",
    "                if child.name == \"iframe\":\n",
    "                    if \"youtube.com\" in str( child.get(\"src\")):\n",
    "                        num_videos += 1\n",
    "                    \n",
    "    text = p.get_text()\n",
    "\n",
    "    if len( text.split()) > 1:\n",
    "        if text not in content:\n",
    "            is_in_exclude_list = False\n",
    "\n",
    "            for exclude_class in exclude_class_list:\n",
    "                if p.has_attr(\"class\") and \\\n",
    "                    exclude_class in p.get_attribute_list( \"class\"):\n",
    "\n",
    "                    is_in_exclude_list = True\n",
    "                    break\n",
    "\n",
    "            for starts_string in exclude_starts_with: \n",
    "                if text.startswith(starts_string):\n",
    "                    is_in_exclude_list = True\n",
    "                    break\n",
    "\n",
    "            if not is_in_exclude_list:\n",
    "                content = content + text + \"\\n\"\n",
    "\n",
    "print( \"num_hrefs : \", num_hrefs)\n",
    "print( \"num_self_hrefs : \", num_self_hrefs)\n",
    "print( \"num_imgs : \", num_imgs)\n",
    "print( \"num_videos : \", num_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_content :  1899\n",
      "average_token_length :  4.20115850447604\n"
     ]
    }
   ],
   "source": [
    "nlp_content = nlp(content)\n",
    "\n",
    "tokens = [token.text for token in nlp_content]\n",
    "                #if not token.is_stop and not token.is_punct]\n",
    "\n",
    "n_tokens_content = len(tokens)\n",
    "\n",
    "print( \"n_tokens_content : \", n_tokens_content)\n",
    "\n",
    "average_token_length = 0\n",
    "for token in tokens:\n",
    "    average_token_length += len(token)\n",
    "\n",
    "average_token_length = average_token_length / n_tokens_content\n",
    "\n",
    "print( \"average_token_length : \", average_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_unique_tokens : 0.3644023170089521\n"
     ]
    }
   ],
   "source": [
    "count_unique_tokens = len( Counter(tokens))\n",
    "n_unique_tokens = len( Counter(tokens)) / n_tokens_content\n",
    "print( \"n_unique_tokens :\", n_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_non_stop_words :  0.4333859926276988\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in nlp_content if not token.is_stop and not token.is_punct]\n",
    "\n",
    "n_non_stop_words = len(tokens) / n_tokens_content\n",
    "\n",
    "print( \"n_non_stop_words : \", n_non_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_non_stop_unique_tokens : 0.7326589595375722\n"
     ]
    }
   ],
   "source": [
    "n_non_stop_unique_tokens = len( Counter(tokens)) / count_unique_tokens\n",
    "print( \"n_non_stop_unique_tokens :\", n_non_stop_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google, funding, startups, enterprise, uncategorized, business, gadgets, google-glass\n",
      "num_keywords :  8\n"
     ]
    }
   ],
   "source": [
    "# Keywords\n",
    "regex_keyword = \"\"\"<meta content=\"(?P<keyword1>[^><\\/\\\"]*)\"\\s[a-zA-Z=\"\\-]*\\sname=\"keywords\".?.?>\"\"\"\n",
    "\n",
    "keywords = \"\"\n",
    "\n",
    "for meta in soup.find_all( 'meta'):\n",
    "    meta_text = str(meta)\n",
    "    match = re.search( regex_keyword, meta_text)\n",
    "    if match:\n",
    "        if not match.group(\"keyword1\") == None:\n",
    "            keywords = match.group(\"keyword1\")\n",
    "            break\n",
    "            \n",
    "# if keywords is not found, take non-stop words from title\n",
    "if keywords == \"\":\n",
    "    tokens = [token.text for token in nlp_title if not token.is_stop and not token.is_punct]\n",
    "    keywords = \", \".join( tokens)\n",
    "\n",
    "print( keywords)\n",
    "\n",
    "num_keywords = len( keywords.split(\", \"))\n",
    "print( \"num_keywords : \", num_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n"
     ]
    }
   ],
   "source": [
    "# Data channel\n",
    "data_channel = \"\"\n",
    "for p in soup.select( \"hgroup\"):\n",
    "    if not p.get(\"data-channel\") == None:\n",
    "        data_channel = p.get(\"data-channel\")\n",
    "        \n",
    "print(data_channel)\n",
    "\n",
    "data_channel = data_channel.lower()\n",
    "\n",
    "data_channel_is_lifestyle = 1 if data_channel == \"culture\" or data_channel == \"\" else 0\n",
    "data_channel_is_entertainment = 1 if data_channel == \"entertainment\" else 0\n",
    "data_channel_is_bus = 1 if data_channel == \"business\" else 0\n",
    "data_channel_is_socmed = 1 if data_channel == \"social-good\" else 0\n",
    "data_channel_is_tech = 1 if data_channel == \"tech\" else 0\n",
    "data_channel_is_world = 1 if data_channel == \"world\" or data_channel == \"u.s.\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title 9\n",
      "n_tokens_content 1899\n",
      "n_unique_tokens 0.3644023170089521\n",
      "n_non_stop_words 0.4333859926276988\n",
      "n_non_stop_unique_tokens 0.7326589595375722\n",
      "num_hrefs 12\n",
      "num_self_hrefs 5\n",
      "num_imgs 1\n",
      "num_videos 1\n",
      "average_token_length 4.20115850447604\n",
      "num_keywords 8\n",
      "data_channel_is_lifestyle 0\n",
      "data_channel_is_entertainment 0\n",
      "data_channel_is_bus 1\n",
      "data_channel_is_socmed 0\n",
      "data_channel_is_tech 0\n",
      "data_channel_is_world 0\n"
     ]
    }
   ],
   "source": [
    "print( \"n_tokens_title\",  n_tokens_title)\n",
    "print( \"n_tokens_content\", n_tokens_content)\n",
    "print( \"n_unique_tokens\", n_unique_tokens)\n",
    "print( \"n_non_stop_words\", n_non_stop_words)\n",
    "print( \"n_non_stop_unique_tokens\", n_non_stop_unique_tokens)\n",
    "print( \"num_hrefs\", num_hrefs)\n",
    "print( \"num_self_hrefs\", num_self_hrefs)\n",
    "print( \"num_imgs\", num_imgs)\n",
    "print( \"num_videos\", num_videos)\n",
    "print( \"average_token_length\", average_token_length)\n",
    "print( \"num_keywords\", num_keywords)\n",
    "print( \"data_channel_is_lifestyle\", data_channel_is_lifestyle)\n",
    "print( \"data_channel_is_entertainment\", data_channel_is_entertainment)\n",
    "print( \"data_channel_is_bus\", data_channel_is_bus)\n",
    "print( \"data_channel_is_socmed\", data_channel_is_socmed)\n",
    "print( \"data_channel_is_tech\", data_channel_is_tech)\n",
    "print( \"data_channel_is_world\", data_channel_is_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data_channel for all the files\n",
    "\n",
    "df_in = pd.read_excel('../data/output/SS_Extracted_content.xlsx')[[\"Id\"]]\n",
    "\n",
    "df_out = pd.DataFrame(columns=[\"Id\", \"data_channel\"])\n",
    "\n",
    "for index, row in df_in.iterrows():\n",
    "    doc_id = row.Id\n",
    "\n",
    "    with open( \"../data/output/html/\" + str(doc_id) + \".html\", \"r\", encoding=\"utf-8\", errors='ignore') as html_file:\n",
    "        html_doc = html_file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    data_channel = \"\"\n",
    "    for p in soup.select( \"hgroup\"):\n",
    "        if not p.get(\"data-channel\") == None:\n",
    "            data_channel = p.get(\"data-channel\")\n",
    "    \n",
    "    df_out.loc[index] = [doc_id, data_channel]\n",
    "    \n",
    "    if index % 100 == 0:\n",
    "        print( index, \", \", end=\"\")\n",
    "    \n",
    "df_out.to_excel('../data/output/0_data_channels.xlsx', index=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the basic features creator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicFeatures import BasicFeaturesCreator as bfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_content = \"\"\n",
    "with open( \"../data/output/html/\" + str(5935) + \".html\", \"r\", encoding=\"utf-8\", errors='ignore') as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "len(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title 9\n",
      "n_tokens_content 1820\n",
      "n_unique_tokens 0.4098901098901099\n",
      "n_non_stop_words 0.4604395604395604\n",
      "n_non_stop_unique_tokens 0.7734584450402144\n",
      "num_hrefs 23\n",
      "num_self_hrefs 2\n",
      "num_imgs 5\n",
      "num_videos 0\n",
      "average_token_length 4.246703296703297\n",
      "num_keywords 6\n",
      "data_channel_is_lifestyle 1\n",
      "data_channel_is_entertainment 0\n",
      "data_channel_is_bus 0\n",
      "data_channel_is_socmed 0\n",
      "data_channel_is_tech 0\n",
      "data_channel_is_world 0\n"
     ]
    }
   ],
   "source": [
    "result = bfc.get_basic_features( html_content)\n",
    "print( \"n_tokens_title\",  result.n_tokens_title)\n",
    "print( \"n_tokens_content\", result.n_tokens_content)\n",
    "print( \"n_unique_tokens\", result.n_unique_tokens)\n",
    "print( \"n_non_stop_words\", result.n_non_stop_words)\n",
    "print( \"n_non_stop_unique_tokens\", result.n_non_stop_unique_tokens)\n",
    "print( \"num_hrefs\", result.num_hrefs)\n",
    "print( \"num_self_hrefs\", result.num_self_hrefs)\n",
    "print( \"num_imgs\", result.num_imgs)\n",
    "print( \"num_videos\", result.num_videos)\n",
    "print( \"average_token_length\", result.average_token_length)\n",
    "print( \"num_keywords\", result.num_keywords)\n",
    "print( \"data_channel_is_lifestyle\", result.data_channel_is_lifestyle)\n",
    "print( \"data_channel_is_entertainment\", result.data_channel_is_entertainment)\n",
    "print( \"data_channel_is_bus\", result.data_channel_is_bus)\n",
    "print( \"data_channel_is_socmed\", result.data_channel_is_socmed)\n",
    "print( \"data_channel_is_tech\", result.data_channel_is_tech)\n",
    "print( \"data_channel_is_world\", result.data_channel_is_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate Basic features of all the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "from BasicFeatures import BasicFeaturesCreator as bfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , \n"
     ]
    }
   ],
   "source": [
    "df_in = pd.read_excel('../data/input/OnlineNewsPopularity.xlsx')[[\"Id\", \" shares\"]]\n",
    "\n",
    "df_out = None\n",
    "out_filename = '../data/output/0_recreated_basic_features.xlsx'\n",
    "\n",
    "if path.exists( out_filename):\n",
    "    df_out = pd.read_excel( out_filename)\n",
    "else:\n",
    "    df_out = pd.DataFrame(columns=[\"Id\", \"n_tokens_title\", \"n_tokens_content\", \"n_unique_tokens\", \\\n",
    "                                   \"n_non_stop_words\", \"n_non_stop_unique_tokens\", \"num_hrefs\", \\\n",
    "                                   \"num_self_hrefs\", \"num_imgs\", \"num_videos\", \"average_token_length\", \\\n",
    "                                   \"num_keywords\", \"data_channel_is_lifestyle\", \\\n",
    "                                   \"data_channel_is_entertainment\", \"data_channel_is_bus\", \\\n",
    "                                   \"data_channel_is_socmed\", \"data_channel_is_tech\", \"data_channel_is_world\", \\\n",
    "                                   \"shares\"])\n",
    "\n",
    "df_out = df_out.astype( {\"Id\":int, \"n_tokens_title\":int, \"n_tokens_content\":int, \"n_unique_tokens\":float, \\\n",
    "                         \"n_non_stop_words\":float, \"n_non_stop_unique_tokens\":float, \"num_hrefs\":int, \\\n",
    "                         \"num_self_hrefs\":int, \"num_imgs\":int, \"num_videos\":int, \"average_token_length\":float, \\\n",
    "                         \"num_keywords\":int, \"data_channel_is_lifestyle\":int, \"data_channel_is_entertainment\":int, \\\n",
    "                         \"data_channel_is_bus\":int, \"data_channel_is_socmed\":int, \"data_channel_is_tech\":int, \\\n",
    "                         \"data_channel_is_world\":int, \"shares\":int})\n",
    "\n",
    "for index, row in df_in.iterrows():\n",
    "    doc_id = row.Id\n",
    "    shares = row[\" shares\"]\n",
    "    \n",
    "    if index % 100 == 0:\n",
    "        print( index, \", \", end=\"\")\n",
    "        \n",
    "    if doc_id in df_out.Id:\n",
    "        #already processed\n",
    "        continue\n",
    "\n",
    "    html_content = \"\"\n",
    "    with open( \"../data/output/html/\" + str(doc_id) + \".html\", \"r\", encoding=\"utf-8\", errors='ignore') as html_file:\n",
    "        html_content = html_file.read()\n",
    "    \n",
    "    result = bfc.get_basic_features( html_content)\n",
    "    \n",
    "    df_out.loc[index] = [doc_id, result.n_tokens_title, result.n_tokens_content, result.n_unique_tokens, \\\n",
    "                            result.n_non_stop_words, result.n_non_stop_unique_tokens, result.num_hrefs, \\\n",
    "                            result.num_self_hrefs, result.num_imgs, result.num_videos, result.average_token_length, \\\n",
    "                            result.num_keywords, result.data_channel_is_lifestyle, \\\n",
    "                            result.data_channel_is_entertainment, result.data_channel_is_bus, \\\n",
    "                            result.data_channel_is_socmed, result.data_channel_is_tech, result.data_channel_is_world, \\\n",
    "                            shares\n",
    "                        ]\n",
    "    \n",
    "    if index % 100 == 0:\n",
    "        df_out.to_excel('../data/output/0_recreated_basic_features.xlsx', index=False)\n",
    "    \n",
    "df_out.to_excel('../data/output/0_recreated_basic_features.xlsx', index=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
